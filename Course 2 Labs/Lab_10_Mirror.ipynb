{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab_10_Mirror.ipynb","provenance":[],"mount_file_id":"1NBRmtWz8F3NIZ_jtMZ-s_OsHH9QEQHSN","authorship_tag":"ABX9TyPOuGuHY+qverHLopYtRtdu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Dowload and prep the data\n","\n","Use the Rock-Paper scissors dataset\n"],"metadata":{"id":"vxXxll0d4sij"}},{"cell_type":"code","source":["import zipfile\n","\n","\n","#extract the archive \n","local_zip = './rps.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('./rps-train')\n","zip_ref.close()\n","\n","local_zip = './rps-test-set.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('./rps-test')"],"metadata":{"id":"8C_uXSTN-uHQ","executionInfo":{"status":"ok","timestamp":1653243985866,"user_tz":420,"elapsed":1699,"user":{"displayName":"Otavio Pailo","userId":"08902513118504543900"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#Assign directory names \n","\n","import os \n","\n","base_dir = './rps-train/rps'\n","\n","rock_dir = os.path.join(base_dir, 'rock')\n","paper_dir = os.path.join(base_dir, 'paper')\n","scissors_dir = os.path.join(base_dir, 'scissors')\n","\n","rock_files = os.listdir(rock_dir)\n","print(rock_files[:10])\n","\n","paper_files = os.listdir(paper_dir)\n","print(paper_files[:10])\n","\n","scissors_files = os.listdir(scissors_dir)\n","print(scissors_files[:10])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7lOuODLA_gBT","executionInfo":{"status":"ok","timestamp":1653243998708,"user_tz":420,"elapsed":188,"user":{"displayName":"Otavio Pailo","userId":"08902513118504543900"}},"outputId":"b2696316-cd05-400b-9676-053f44bbf776"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['rock04-008.png', 'rock07-k03-095.png', 'rock01-067.png', 'rock01-079.png', 'rock04-041.png', 'rock06ck02-043.png', 'rock02-096.png', 'rock07-k03-055.png', 'rock04-070.png', 'rock02-111.png']\n","['paper04-056.png', 'paper04-116.png', 'paper06-061.png', 'paper06-065.png', 'paper01-108.png', 'paper07-104.png', 'paper05-005.png', 'paper05-082.png', 'paper03-075.png', 'paper03-097.png']\n","['testscissors02-105.png', 'testscissors03-017.png', 'testscissors03-045.png', 'scissors01-070.png', 'testscissors03-093.png', 'testscissors03-010.png', 'testscissors03-036.png', 'testscissors02-081.png', 'scissors01-061.png', 'testscissors01-115.png']\n"]}]},{"cell_type":"markdown","source":["## Build the model \n","\n","For the CNN, you will use 4 convolution layers with 64, 64, 128, 128 filters \n","\n","The output layer will be 3 neuron dense Softmax (scales the output probabilites so that they all add up to 1)\n","\n","The order of the output will be `paper`-`rock`-`scissors`"],"metadata":{"id":"Fo_Z1vAkAHeN"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers \n","\n","model = tf.keras.models.Sequential([\n","        #Input shape is the desired size of the image with 3 bytes \n","        #First convolution\n","        layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n","        layers.MaxPooling2D(2, 2),\n","        #Second convolution\n","        layers.Conv2D(64, (3,3), activation='relu'),\n","        layers.MaxPooling2D(2,2),\n","        #Third convolution \n","        layers.Conv2D(128, (3,3), activation='relu'),\n","        layers.MaxPooling2D(2,2),\n","        #Fourth convolution \n","        layers.Conv2D(128, (3,3), activation='relu'),\n","        layers.MaxPooling2D(2,2),\n","        #Flatten \n","        layers.Flatten(),\n","        #Dropout layer\n","        layers.Dropout(0.5),\n","        #512 neuron hidden layer \n","        layers.Dense(512, activation='relu'),\n","        #Output layer \n","        layers.Dense(3, activation='softmax')\n","\n","])\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kps9kSL7A8uj","executionInfo":{"status":"ok","timestamp":1653244427671,"user_tz":420,"elapsed":383,"user":{"displayName":"Otavio Pailo","userId":"08902513118504543900"}},"outputId":"24d7c132-10b3-4bdc-d961-68ee914c2723"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_15 (Conv2D)          (None, 148, 148, 64)      1792      \n","                                                                 \n"," max_pooling2d_13 (MaxPoolin  (None, 74, 74, 64)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_16 (Conv2D)          (None, 72, 72, 64)        36928     \n","                                                                 \n"," max_pooling2d_14 (MaxPoolin  (None, 36, 36, 64)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_17 (Conv2D)          (None, 34, 34, 128)       73856     \n","                                                                 \n"," max_pooling2d_15 (MaxPoolin  (None, 17, 17, 128)      0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_18 (Conv2D)          (None, 15, 15, 128)       147584    \n","                                                                 \n"," max_pooling2d_16 (MaxPoolin  (None, 7, 7, 128)        0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_2 (Flatten)         (None, 6272)              0         \n","                                                                 \n"," dropout_2 (Dropout)         (None, 6272)              0         \n","                                                                 \n"," dense_2 (Dense)             (None, 512)               3211776   \n","                                                                 \n"," dense_3 (Dense)             (None, 3)                 1539      \n","                                                                 \n","=================================================================\n","Total params: 3,473,475\n","Trainable params: 3,473,475\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Next compile the model but change the `loss` function to `categorical_crossentropy` instead of `binary_crossentropy`"],"metadata":{"id":"2D2vfGGhDAaL"}},{"cell_type":"code","source":["#Set the training parameters\n","\n","model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"],"metadata":{"id":"J8xeiaCcDKxO","executionInfo":{"status":"ok","timestamp":1653244923749,"user_tz":420,"elapsed":1,"user":{"displayName":"Otavio Pailo","userId":"08902513118504543900"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#Prep the image data generators \n","\n","from keras_preprocessing.image import ImageDataGenerator\n","\n","TRAINING_DIR = \"./rps-train/rps\"\n","training_datagen = ImageDataGenerator(\n","    rescale = 1/255,\n","    rotation_range = 40,\n","    width_shift_range = 0.2,\n","    height_shift_range = 0.2,\n","    shear_range = 0.2,\n","    zoom_range = 0.2,\n","    horizontal_flip = True,\n","    fill_mode = 'nearest')\n","\n","VALIDATION_DIR = './rps-test/rps-test-set'\n","validation_datagen = ImageDataGenerator(rescale = 1/255)\n","\n","train_generator = training_datagen.flow_from_directory(\n","    TRAINING_DIR,\n","    target_size=(150,150),\n","    class_mode='categorical',\n","    batch_size=126\n",")\n","\n","\n","validaiton_generator = validation_datagen.flow_from_directory(\n","    VALIDATION_DIR, \n","    target_size=(150,150),\n","    class_mode='categorical',\n","    batch_size=126\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vgbXwGOiDWoJ","executionInfo":{"status":"ok","timestamp":1653244927355,"user_tz":420,"elapsed":330,"user":{"displayName":"Otavio Pailo","userId":"08902513118504543900"}},"outputId":"2a9b7e02-83a2-41be-e179-bef3558be8e7"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2520 images belonging to 3 classes.\n","Found 372 images belonging to 3 classes.\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","\n","#Creating a callback \n","\n","class myCallback(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs={}):\n","\n","    #check accuracy\n","    accuracy_value = 0.99\n","    if(logs.get('val_accuracy') > accuracy_value):\n","\n","      print(f'\\nAccuracy is more than {accuracy_value} so cancelling training.')\n","      self.model.stop_training = True\n","\n","callbacks = myCallback()"],"metadata":{"id":"sPb9hjTgbl3M","executionInfo":{"status":"ok","timestamp":1653250982522,"user_tz":420,"elapsed":155,"user":{"displayName":"Otavio Pailo","userId":"08902513118504543900"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["#Train the model \n","\n","history = model.fit(\n","    train_generator,\n","    epochs=20,\n","    steps_per_epoch=14, \n","    validation_data = validaiton_generator,\n","    verbose = 1,\n","    validation_steps = 3,\n","    callbacks=[callbacks]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7u51GUFYE6Ia","executionInfo":{"status":"ok","timestamp":1653251790368,"user_tz":420,"elapsed":801702,"user":{"displayName":"Otavio Pailo","userId":"08902513118504543900"}},"outputId":"72309c1e-b61a-4a22-eedb-d89a1890442d"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","14/14 [==============================] - 130s 9s/step - loss: 0.0574 - accuracy: 0.9790 - val_loss: 0.1040 - val_accuracy: 0.9247\n","Epoch 2/20\n","14/14 [==============================] - 129s 9s/step - loss: 0.1180 - accuracy: 0.9660 - val_loss: 0.0609 - val_accuracy: 0.9785\n","Epoch 3/20\n","14/14 [==============================] - 129s 9s/step - loss: 0.0472 - accuracy: 0.9841 - val_loss: 0.0726 - val_accuracy: 0.9651\n","Epoch 4/20\n","14/14 [==============================] - 129s 9s/step - loss: 0.2331 - accuracy: 0.9308 - val_loss: 0.0818 - val_accuracy: 0.9785\n","Epoch 5/20\n","14/14 [==============================] - 129s 9s/step - loss: 0.0718 - accuracy: 0.9768 - val_loss: 0.0686 - val_accuracy: 0.9812\n","Epoch 6/20\n","14/14 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9853\n","Accuracy is more than 0.99 so cancelling training.\n","14/14 [==============================] - 129s 9s/step - loss: 0.0483 - accuracy: 0.9853 - val_loss: 0.0116 - val_accuracy: 1.0000\n"]}]}]}