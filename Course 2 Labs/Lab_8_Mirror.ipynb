{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":950,"status":"ok","timestamp":1652913227397,"user":{"displayName":"Otavio Pailo","userId":"08902513118504543900"},"user_tz":420},"id":"lLhz34KgV168"},"outputs":[],"source":["import os \n","import zipfile\n","\n","#extract the archive \n","zip_ref = zipfile.ZipFile('./horse-or-human.zip')\n","zip_ref.extractall('tmp/horse-or-human')\n","\n","zip_ref = zipfile.ZipFile('./validation-horse-or-human.zip', 'r')\n","zip_ref.extractall('tmp/validation-horse-or-human')\n","\n","zip_ref.close()\n","\n","#directories with training horse and humanpictures \n","train_horse_dir = os.path.join('tmp/horse-or-human/horses')\n","train_human_dir = os.path.join('tmp/horse-or-human/humans')\n","\n","# Directory with validation horse pictures\n","validation_horse_dir = os.path.join('tmp/validation-horse-or-human/horses')\n","\n","# Directory with validation human pictures\n","validation_human_dir = os.path.join('tmp/validation-horse-or-human/humans')\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3578,"status":"ok","timestamp":1652913232369,"user":{"displayName":"Otavio Pailo","userId":"08902513118504543900"},"user_tz":420},"id":"JT_4ljaEoe7I"},"outputs":[],"source":["import tensorflow as tf\n","\n","model = tf.keras.models.Sequential([\n","      #Input shape is the desired size of the image 300x300 with 3bytes \n","      tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape = (300, 300, 3)),\n","      tf.keras.layers.MaxPooling2D(2, 2),\n","      tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n","      tf.keras.layers.MaxPooling2D(2, 2),\n","      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","      tf.keras.layers.MaxPooling2D(2, 2),\n","      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","      tf.keras.layers.MaxPooling2D(2,2),\n","      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","      tf.keras.layers.MaxPooling2D(2,2),\n","\n","      tf.keras.layers.Flatten(),\n","      tf.keras.layers.Dense(512, activation='relu'),\n","      tf.keras.layers.Dense(1, activation='sigmoid')\n","])"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":159,"status":"ok","timestamp":1652913300311,"user":{"displayName":"Otavio Pailo","userId":"08902513118504543900"},"user_tz":420},"id":"0mtic0nuTmBf"},"outputs":[],"source":["from tensorflow.keras.optimizers import RMSprop\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer=RMSprop(learning_rate=1e-4),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":404,"status":"ok","timestamp":1652913749392,"user":{"displayName":"Otavio Pailo","userId":"08902513118504543900"},"user_tz":420},"id":"vSFaJxSoT2i0","outputId":"e8ef8041-917d-48d2-f3a1-d63d4bb2384e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1027 images belonging to 2 classes.\n","Found 256 images belonging to 2 classes.\n"]}],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","train_datagen = ImageDataGenerator(\n","    rescale = 1/255,\n","    rotation_range = 40,\n","    width_shift_range = 0.2,\n","    height_shift_range = 0.2,\n","    shear_range = 0.2,\n","    zoom_range = 0.2,\n","    horizontal_flip = True,\n","    fill_mode = 'nearest'\n",")\n","\n","validation_datagen = ImageDataGenerator(rescale = 1/255)\n","\n","#Flow training images in batches of 128 using train_datagen generator \n","train_generator = train_datagen.flow_from_directory(\n","    'tmp/horse-or-human/', #source directory for training images\n","    target_size = (300, 300), # Images will be resized to 150 x 150\n","    batch_size = 128,\n","    class_mode = 'binary'\n",")\n","\n","#Flow training images in batches of 32 using train_datagen generator \n","validation_generator = validation_datagen.flow_from_directory(\n","    'tmp/validation-horse-or-human', #This is the source directory for the validation images\n","    target_size = (300, 300), #All images will be resized to 150x150\n","    batch_size = 32,\n","    class_mode = 'binary'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"AQ8pmaNYVkrV"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","8/8 [==============================] - 99s 12s/step - loss: 0.7069 - accuracy: 0.5172 - val_loss: 0.6812 - val_accuracy: 0.5156\n","Epoch 2/20\n","8/8 [==============================] - 95s 11s/step - loss: 0.6799 - accuracy: 0.5829 - val_loss: 0.6882 - val_accuracy: 0.5000\n","Epoch 3/20\n","8/8 [==============================] - 95s 11s/step - loss: 0.6605 - accuracy: 0.6296 - val_loss: 0.6857 - val_accuracy: 0.5000\n","Epoch 4/20\n","8/8 [==============================] - 95s 12s/step - loss: 0.6649 - accuracy: 0.6418 - val_loss: 0.6326 - val_accuracy: 0.5938\n","Epoch 5/20\n","8/8 [==============================] - 95s 11s/step - loss: 0.6133 - accuracy: 0.6763 - val_loss: 0.6240 - val_accuracy: 0.5703\n","Epoch 6/20\n","8/8 [==============================] - 94s 11s/step - loss: 0.5994 - accuracy: 0.6763 - val_loss: 0.7342 - val_accuracy: 0.5000\n","Epoch 7/20\n","8/8 [==============================] - 94s 11s/step - loss: 0.5865 - accuracy: 0.6974 - val_loss: 0.7701 - val_accuracy: 0.5000\n","Epoch 8/20\n","8/8 [==============================] - 94s 11s/step - loss: 0.5654 - accuracy: 0.7164 - val_loss: 0.6961 - val_accuracy: 0.5156\n","Epoch 9/20\n","8/8 [==============================] - 94s 12s/step - loss: 0.5608 - accuracy: 0.7041 - val_loss: 0.7732 - val_accuracy: 0.5039\n","Epoch 10/20\n","8/8 [==============================] - 94s 11s/step - loss: 0.5235 - accuracy: 0.7464 - val_loss: 0.7041 - val_accuracy: 0.5391\n","Epoch 11/20\n","8/8 [==============================] - 94s 11s/step - loss: 0.5077 - accuracy: 0.7675 - val_loss: 0.7166 - val_accuracy: 0.5469\n","Epoch 12/20\n","8/8 [==============================] - 93s 13s/step - loss: 0.5253 - accuracy: 0.7430 - val_loss: 0.8645 - val_accuracy: 0.5273\n","Epoch 13/20\n","8/8 [==============================] - 94s 11s/step - loss: 0.4976 - accuracy: 0.7464 - val_loss: 0.7078 - val_accuracy: 0.5898\n","Epoch 14/20\n","8/8 [==============================] - 92s 11s/step - loss: 0.4625 - accuracy: 0.7864 - val_loss: 0.4116 - val_accuracy: 0.7422\n","Epoch 15/20\n","8/8 [==============================] - 93s 11s/step - loss: 0.5880 - accuracy: 0.7419 - val_loss: 1.0815 - val_accuracy: 0.5352\n","Epoch 16/20\n","8/8 [==============================] - 94s 11s/step - loss: 0.4201 - accuracy: 0.8065 - val_loss: 1.3446 - val_accuracy: 0.5117\n","Epoch 17/20\n","8/8 [==============================] - 93s 11s/step - loss: 0.4476 - accuracy: 0.7809 - val_loss: 1.1438 - val_accuracy: 0.5352\n","Epoch 18/20\n","8/8 [==============================] - 94s 11s/step - loss: 0.4091 - accuracy: 0.8109 - val_loss: 0.5590 - val_accuracy: 0.7227\n","Epoch 19/20\n","8/8 [==============================] - 94s 11s/step - loss: 0.4542 - accuracy: 0.7831 - val_loss: 1.2059 - val_accuracy: 0.5469\n","Epoch 20/20\n","8/8 [==============================] - 93s 13s/step - loss: 0.4362 - accuracy: 0.7864 - val_loss: 0.9675 - val_accuracy: 0.5859\n"]}],"source":["EPOCHS = 20\n","\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch = 8,\n","    epochs = EPOCHS,\n","    verbose = 1,\n","    validation_data = validation_generator,\n","    validation_steps = 8\n",")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPKoR1RJPqdcOl44qldSG0w","name":"Lab_8_Mirror.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}